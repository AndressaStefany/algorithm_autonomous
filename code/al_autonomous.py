# -*- coding: utf-8 -*-
"""al_autonomous.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zf4Lf8EZPzRV1wEgyYhXXaBAm_M3hp1d

# Boot
"""

# from google.colab import drive
# drive.mount("/gdrive", force_remount=True)

!cp -r '/gdrive/My Drive/Mestrado/07 - Autonomous/code/' '/content'
!mv '/content/code' '/content/scripts'
# !cp '/content/scripts/cluster.py' '/content'
# !cp '/content/scripts/radius.py' '/content'
# !cp '/content/scripts/new_cluster.py' '/content'
# !cp '/content/scripts/update_clusters.py' '/content'
# !cp '/content/scripts/utils.py' '/content'
# !cp '/content/scripts/overlap.py' '/content'
# !cp '/content/scripts/merge.py' '/content'
# !cp '/content/scripts/volume.py' '/content'
!cp '/content/scripts/split.py' '/content'
!rm -r '/content/scripts/'

# !cp -r '/gdrive/My Drive/Mestrado/07 - Autonomous/data/' '/content'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import distance
from tqdm import tqdm

# to see whatâ€™s spending the most time
# python3 -m cProfile -s cumtime al_autonomous.py > a1.txt

"""# Colors"""

# # random list of colors
# import random
# from sklearn.utils import shuffle

# list_cor = []
# for i in range(0,10000):
#     r = lambda: random.randint(0,255)
#     cor = '#%02X%02X%02X' % (r(),r(),r())
#     list_cor.append(cor)

# unique_list = np.array(list_cor)
# unique_list = np.unique(unique_list)
# print(len(unique_list))
# print(len(list_cor))
# list_cor = unique_list

# list_cor = shuffle(list_cor)

# barWidth = 1

# plt.figure(figsize=(10,5))
# for i in range(0,len(list_cor)):
#     plt.bar(i, 10, color = list_cor[i], width=barWidth, label=str(list_cor[i]))
# plt.show()

"""# Algorithm"""

from utils import min_dist, dist
from new_cluster import new_cluster
from update_clusters import update_winner_cluster, update_nearest_cluster
from overlap import overlap
from merge import merge
from radius import get_radius
from volume import get_volume
from split import split

class Autonomous:
    def __init__(self, fac, frac=100, m=4): # default of article
        self.fac = fac
        self.frac = frac
        self.m = m
        self.clusters = []

    pass

    # input x = np.array
    def process(self, x):
        p = len(x)

        # the first cluster
        if len(self.clusters) == 0:
            self.clusters.append(new_cluster(
                x, self.frac, self.fac, p, self.m))

        else:
            # elect winning cluster
            win_cluster = min_dist(x, self.clusters)
            win_cluster_radius = get_radius(
                self.fac, p, self.m, win_cluster.k)

            # point is not contained in the winning cluster
            if (distance.mahalanobis(x, win_cluster.centroid, win_cluster.inv_cov) > win_cluster_radius):
                self.clusters.append(new_cluster(
                    x, self.frac, self.fac, p, self.m))

            else:
                self.clusters.remove(win_cluster)
                # update winning cluster
                update_winner_cluster(x, win_cluster)

                if len(self.clusters) != 0:
                    # update nearest neighbor of winning cluster
                    update_nearest_cluster(x, win_cluster, self.clusters)

                # check overlap
                (max_olap, max_cluster) = overlap(
                    win_cluster, self.clusters)
                if (max_olap > 0):
                    cluster_merged = merge(win_cluster, max_cluster)

                    V_m = get_volume(cluster_merged, self.fac, p, self.m)
                    V_w = get_volume(win_cluster, self.fac, p, self.m)
                    V_c = get_volume(max_cluster, self.fac, p, self.m)
                    # Check volume, for the final decision
                    if V_m <= p*(V_w + V_c):
                        self.clusters.remove(max_cluster)
                        self.clusters.append(cluster_merged)
                    else:
                        split(self.clusters, win_cluster)
        pass

"""# Test with S1

http://cs.joensuu.fi/sipu/datasets/
"""

data_s1 = pd.read_csv('data/s1.txt', delimiter='    ', header=None, engine='python')
print(data_s1.values)
print(type(data_s1.values))
print(len(data_s1.values))

data = data_s1
normalized_df=(data-data.min())/(data.max()-data.min())

data_s1_cb = pd.read_csv('data/s1-cb.txt', delimiter=' ', header=None, engine='python')
data_s1_cb = data_s1_cb.drop([2], axis=1)
print(len(data_s1_cb.values))

autonomous_S1 = Autonomous(fac = 3.3)
for r in tqdm(data_s1.values):
    autonomous_S1.process(r)

len(autonomous_S1.clusters)

"""## Plot"""

from sklearn import mixture

i = 0
while len(autonomous_S1.clusters[i].S) < 14:
    i += 1

X = autonomous_S1.clusters[i].S
gmm = mixture.GaussianMixture(n_components=2, covariance_type='full').fit(X)

data = pd.DataFrame(autonomous_S1.clusters[i].S)

plt.figure()
plt.scatter(data.iloc[:][0], data[:][1])
plt.show()

from matplotlib.colors import LogNorm

# display predicted scores by the model as a contour plot
x = np.linspace(min(data_s1.iloc[:][0]), max(data_s1.iloc[:][0]))
y = np.linspace(min(data_s1.iloc[:][1]), max(data_s1.iloc[:][1]))
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = -gmm.score_samples(XX)
Z = Z.reshape(X.shape)

CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),
                 levels=np.logspace(0, 3, 10))
CB = plt.colorbar(CS, shrink=0.8, extend='both')
plt.scatter(data_s1[:][0], data_s1[:][1], 2.0)

plt.title('Negative log-likelihood predicted by a GMM')
plt.axis('tight')
plt.show()